import pandas as pd
import numpy as np
import joblib
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer, TransformedTargetRegressor
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import os

# === Constantes ===
CONVERSION_RATE = 0.5  # roupie -> MAD
CURRENT_YEAR = 2025
OUTPUT_DIR = "visualizations"
MODEL_DIR = "../models"

os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(MODEL_DIR, exist_ok=True)


# === Fonctions principales ===

def load_data(filepath: str) -> pd.DataFrame:
    """Charger et nettoyer les donnÃ©es"""
    print("ğŸ“¥ Chargement des donnÃ©es...")
    df = pd.read_csv(filepath).drop_duplicates().dropna()
    print(f"âœ… DonnÃ©es chargÃ©es : {df.shape[0]} lignes, {df.shape[1]} colonnes")
    return df


def convert_prices(df: pd.DataFrame) -> pd.DataFrame:
    """Convertir la colonne du prix en MAD"""
    df["selling_price"] *= CONVERSION_RATE
    print(f"ğŸ’° Prix moyen (MAD) : {df['selling_price'].mean():,.2f}")
    return df


def remove_outliers(df: pd.DataFrame, cols=None) -> pd.DataFrame:
    """Supprimer les outliers via IQR"""
    print("\nğŸ§¹ Suppression des outliers...")
    cols = cols or ["selling_price", "year", "max_power_bhp", "torque_nm"]
    for col in [c for c in cols if c in df.columns]:
        Q1, Q3 = df[col].quantile([0.25, 0.75])
        IQR = Q3 - Q1
        df = df[(df[col] >= Q1 - 1.5 * IQR) & (df[col] <= Q3 + 1.5 * IQR)]
    print(f"âœ… AprÃ¨s suppression des outliers : {df.shape[0]} lignes restantes")
    return df


def create_features(df: pd.DataFrame) -> pd.DataFrame:
    """CrÃ©er des variables dÃ©rivÃ©es"""
    print("\nğŸ§© CrÃ©ation des features dÃ©rivÃ©es...")
    if "year" in df.columns:
        df["vehicle_age"] = CURRENT_YEAR - df["year"]
    print("âœ… Features dÃ©rivÃ©es crÃ©Ã©es.")
    return df


def build_pipeline(num_cols, cat_cols):
    """CrÃ©er le pipeline de prÃ©traitement et le modÃ¨le"""
    preprocessor = ColumnTransformer([
        ("num", StandardScaler(), num_cols),
        ("cat", OneHotEncoder(handle_unknown="ignore", sparse_output=False), cat_cols)
    ])
    base_pipeline = Pipeline([
        ("preprocessor", preprocessor),
        ("model", RandomForestRegressor(n_estimators=100, max_depth=15, random_state=42, n_jobs=-1))
    ])
    # Transformation log sur la variable cible
    return TransformedTargetRegressor(regressor=base_pipeline, func=np.log1p, inverse_func=np.expm1)


def evaluate_model(model, X_train, X_test, y_train, y_test):
    """Ã‰valuer le modÃ¨le"""
    preds_train = model.predict(X_train)
    preds_test = model.predict(X_test)

    def metrics(y, y_pred):
        return {
            "RMSE": np.sqrt(mean_squared_error(y, y_pred)),
            "MAE": mean_absolute_error(y, y_pred),
            "R2": r2_score(y, y_pred)
        }

    results = {"train": metrics(y_train, preds_train), "test": metrics(y_test, preds_test)}

    print("\nğŸ“Š Performances du modÃ¨le :")
    print(f"  ğŸŸ¢ RÂ² (train): {results['train']['R2']:.3f}")
    print(f"  ğŸ”µ RÂ² (test) : {results['test']['R2']:.3f}")
    print(f"  ğŸ“‰ RMSE (test): {results['test']['RMSE']:.2f}")
    print(f"  ğŸ“ MAE (test) : {results['test']['MAE']:.2f}")

    return results


def plot_feature_importance(model, num_cols, cat_cols):
    """Afficher et sauvegarder le graphique d'importance des features"""
    print("\nğŸ“ˆ GÃ©nÃ©ration du graphique d'importance des features...")

    importances = model.regressor_.named_steps["model"].feature_importances_
    ohe = model.regressor_.named_steps["preprocessor"].named_transformers_["cat"]

    cat_feature_names = ohe.get_feature_names_out(cat_cols) if cat_cols else []
    feature_names = np.concatenate([num_cols, cat_feature_names])

    indices = np.argsort(importances)[::-1]
    feature_names_sorted = np.array(feature_names)[indices]
    importances_sorted = importances[indices]

    top_n = 20
    plt.figure(figsize=(10, 6))
    plt.barh(feature_names_sorted[:top_n][::-1], importances_sorted[:top_n][::-1], color='teal')
    plt.xlabel("Importance")
    plt.ylabel("Feature")
    plt.title("Importance des Variables - Random Forest")
    plt.tight_layout()
    plt.savefig(f"{OUTPUT_DIR}/feature_importance.png", dpi=150)
    plt.show()
    print(f"Graphique sauvegardÃ© : {OUTPUT_DIR}/feature_importance.png")


def main():
    """Pipeline complet d'entraÃ®nement"""
    print("\n" + "=" * 70)
    print("ğŸš— DÃ‰MARRAGE DU PIPELINE - CarPriceML")
    print("=" * 70)

    # Chargement et prÃ©paration
    df = load_data("../data/car-details.csv")
    df = convert_prices(df)
    df = remove_outliers(df)
    df = create_features(df)

    # SÃ©paration features / target
    #X = df.drop(columns=["selling_price", "name", "seats", "km_driven"], errors="ignore")
    cols=['vehicle_age','year','max_power_bhp','torque_nm','engine_cc']
    X=df[cols]
    y = df["selling_price"]

    num_cols = X.select_dtypes(include=["int64", "float64"]).columns.tolist()
    cat_cols = X.select_dtypes(include=["object", "category"]).columns.tolist()

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )
    print(f"ğŸ“Š Jeu d'entraÃ®nement : {X_train.shape}, Test : {X_test.shape}")

    # CrÃ©ation et entraÃ®nement
    model = build_pipeline(num_cols, cat_cols)
    model.fit(X_train, y_train)

    # Ã‰valuation
    results = evaluate_model(model, X_train, X_test, y_train, y_test)

    # Visualisation des features importantes
    plot_feature_importance(model, num_cols, cat_cols)

    # Sauvegarde
    joblib.dump(model, f"{MODEL_DIR}/rf_model.joblib")
    joblib.dump({"num_cols": num_cols, "cat_cols": cat_cols}, f"{MODEL_DIR}/feature_info.joblib")
    print(f"ğŸ’¾ ModÃ¨le sauvegardÃ© dans : {MODEL_DIR}/rf_model.joblib")

    print("\nâœ… Pipeline terminÃ© avec succÃ¨s !")
    return model, results


if __name__ == "__main__":
    main()